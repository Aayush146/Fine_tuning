{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b5165f-af96-4178-a017-1390ce19d6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (24.0)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\nDownloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m79.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m49.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.0\n    Uninstalling pip-24.0:\n      Successfully uninstalled pip-24.0\nSuccessfully installed pip-24.3.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f87f29e-9075-4145-89e2-42fef435e266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2608d3c-dfb7-4683-b395-8c72c6067be5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (1.2.1)\nRequirement already satisfied: peft in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (0.14.0)\nRequirement already satisfied: bitsandbytes in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (0.45.0)\nRequirement already satisfied: transformers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (4.47.1)\nRequirement already satisfied: trl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (0.13.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.12/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.12/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (2.4.0+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (from accelerate) (0.27.0)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from peft) (4.66.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /databricks/python3/lib/python3.12/site-packages (from bitsandbytes) (4.11.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.32.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (from transformers) (0.21.0)\nRequirement already satisfied: datasets>=2.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages (from trl) (3.2.0)\nRequirement already satisfied: rich in /databricks/python3/lib/python3.12/site-packages (from trl) (13.3.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /databricks/python3/lib/python3.12/site-packages (from datasets>=2.21.0->trl) (15.0.2)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from datasets>=2.21.0->trl) (1.5.3)\nRequirement already satisfied: xxhash in /databricks/python3/lib/python3.12/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /databricks/python3/lib/python3.12/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /databricks/python3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2023.5.0)\nRequirement already satisfied: aiohttp in /databricks/python3/lib/python3.12/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (74.0.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.2.65)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.2.0.44)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.5.119)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.6.0.99)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.3.0.142)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\nRequirement already satisfied: triton==3.0.0 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\nRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /databricks/python3/lib/python3.12/site-packages (from rich->trl) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich->trl) (2.15.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.3)\nRequirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->trl) (0.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.21.0->trl) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89b2ca4-9491-45d2-bc68-793dac92e13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 17:35:44.686584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1736098544.697974   22775 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1736098544.701502   22775 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-05 17:35:44.714621: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser, TrainingArguments, pipeline, logging)\n",
    "from peft import PeftModel, LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig ## the actual trainer class\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] =\"expandable_segments:True\"\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c1a75d-6ce9-4bf0-92d1-fe565dad7645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:45: UserWarning: The cache_dir for this dataset is /root/.cache, which is not a persistent path.Therefore, if/when the cluster restarts, the downloaded dataset will be lost.The persistent storage options for this workspace/cluster config are: [DBFS].Please update either `cache_dir` or the environment variable `HF_DATASETS_CACHE`to be under one of the following root directories: ['/dbfs/']\n  warnings.warn(warning_message)\n/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:14: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nYour GPU supports bfloat16: accelerate training with bf16=True\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851d6bd3-eb7d-4c05-9162-b734341b5f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499.042\n"
     ]
    }
   ],
   "source": [
    "running_sum = 0\n",
    "for i in dataset['text']:\n",
    "    running_sum += len(i)\n",
    "print(running_sum/len(dataset['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36b9b97-80bb-4fc3-acfa-5d7ca24ec81a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b39dce-4f50-49f3-a496-05684867b1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11443\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = dataset['text']\n",
    "context_lengths = np.asarray([len(i) for i in texts])\n",
    "print(max(context_lengths))\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(context_lengths, bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde59f3b-0574-4297-a3e6-2a9cf14bfd4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_questions = []\n",
    "assistant_answers = []\n",
    "charac_1 = \"[INST]\"\n",
    "charac_2 = \"[/INST]\"\n",
    "\n",
    "for i in dataset['text']:\n",
    "    index_1 = i.find(charac_1)\n",
    "    index_2 = i.find(charac_2)\n",
    "    user_questions.append(i[index_1:index_2])\n",
    "    assistant_answers.append(i[index_2:].replace(\"</s>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d5aa2e-59a3-4a56-ae3c-be5959f964f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_lengths = np.asarray([len(i) for i in user_questions])\n",
    "assistant_lengths = np.asarray([len(i) for i in assistant_answers])\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(user_lengths, bins=50, kde=True, color= 'red')\n",
    "plt.title('Distribution of user questions')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8bd762-641d-4042-ade8-fa2af09a5bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10869\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(assistant_lengths, bins=50, kde=True, color = 'green')\n",
    "print(max(assistant_lengths))\n",
    "plt.title('Distribution of model answers')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0da26d-11ff-450b-9ed6-fe45133e3a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lets define the fine tuning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "638c51c3-33cd-4d7d-918e-1875da6d012a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization process (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 5\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c295f44-464c-4e0c-ad73-ca4261751398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5621fa31-a467-4a04-a92e-9d303971ceee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5168a4a4db9e4f6a9648051bf8e9e771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## get quantised model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1226a8-61bc-41dc-9fb8-3ad74cddb856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80acd9ff-f179-49b2-8f08-71eb0b3850a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = SFTConfig(\n",
    "    per_device_train_batch_size= per_device_train_batch_size,\n",
    "    gradient_accumulation_steps= gradient_accumulation_steps,\n",
    "    warmup_steps = 5,\n",
    "    logging_steps = logging_steps,\n",
    "    num_train_epochs = num_train_epochs, # Set this for 1 full training run.\n",
    "    learning_rate = learning_rate,\n",
    "    fp16 = fp16,\n",
    "    bf16 = bf16,\n",
    "    optim = optim,\n",
    "    weight_decay = weight_decay,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    "    output_dir = output_dir,\n",
    "    report_to = \"tensorboard\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = packing, # Can make training 5x faster for short sequences.\n",
    "    dataset_text_field=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7b17fe-3541-46e9-aee0-50e774be81e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 10%\r\rReading package lists... Done\r\r\n\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... Done\r\r\n\rReading state information... 0% \r\rReading state information... 0%\r\rReading state information... Done\r\r\nE: Unable to locate package libaio-dev\r\n"
     ]
    }
   ],
   "source": [
    "# On Ubuntu/Debian:\n",
    "!sudo apt-get install libaio-dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528cd9dc-74d5-484f-83ff-5031d79d68d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-05 17:36:28,723] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n\u001B[93m [WARNING] \u001B[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @autocast_custom_fwd\n/databricks/python/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @autocast_custom_bwd\n[rank0]:[W105 17:36:33.863942996 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/500 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-557281251197422>, line 11\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SFTTrainer(\n",
       "\u001B[1;32m      3\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n",
       "\u001B[1;32m      4\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mdataset,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_arguments,\n",
       "\u001B[1;32m      8\u001B[0m )\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n",
       "\u001B[0;32m---> 11\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:460\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m    442\u001B[0m     active_session_failed\n",
       "\u001B[1;32m    443\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m autologging_is_disabled(autologging_integration)\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    454\u001B[0m     \u001B[38;5;66;03m# warning behavior during original function execution, since autologging is being\u001B[39;00m\n",
       "\u001B[1;32m    455\u001B[0m     \u001B[38;5;66;03m# skipped\u001B[39;00m\n",
       "\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n",
       "\u001B[1;32m    457\u001B[0m         disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    458\u001B[0m         reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    459\u001B[0m     ):\n",
       "\u001B[0;32m--> 460\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    462\u001B[0m \u001B[38;5;66;03m# Whether or not the original / underlying function has been called during the\u001B[39;00m\n",
       "\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# execution of patched code\u001B[39;00m\n",
       "\u001B[1;32m    464\u001B[0m original_has_been_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/huggingface_patches/transformers.py:54\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_fit_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     52\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 54\u001B[0m     model \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     55\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/trainer.py:2164\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   2162\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n",
       "\u001B[1;32m   2163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 2164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n",
       "\u001B[1;32m   2165\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n",
       "\u001B[1;32m   2166\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n",
       "\u001B[1;32m   2167\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n",
       "\u001B[1;32m   2168\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n",
       "\u001B[1;32m   2169\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/trainer.py:2524\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n",
       "\u001B[1;32m   2517\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m   2518\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n",
       "\u001B[1;32m   2519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m   2520\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n",
       "\u001B[1;32m   2521\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n",
       "\u001B[1;32m   2522\u001B[0m )\n",
       "\u001B[1;32m   2523\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n",
       "\u001B[0;32m-> 2524\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n",
       "\u001B[1;32m   2526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   2527\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n",
       "\u001B[1;32m   2528\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n",
       "\u001B[1;32m   2529\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n",
       "\u001B[1;32m   2530\u001B[0m ):\n",
       "\u001B[1;32m   2531\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n",
       "\u001B[1;32m   2532\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/trainer.py:3654\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n",
       "\u001B[1;32m   3651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n",
       "\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n",
       "\u001B[0;32m-> 3654\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs, num_items_in_batch\u001B[38;5;241m=\u001B[39mnum_items_in_batch)\n",
       "\u001B[1;32m   3656\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n",
       "\u001B[1;32m   3657\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   3658\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   3659\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
       "\u001B[1;32m   3660\u001B[0m ):\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/trainer.py:3708\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n",
       "\u001B[1;32m   3706\u001B[0m         loss_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n",
       "\u001B[1;32m   3707\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs}\n",
       "\u001B[0;32m-> 3708\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n",
       "\u001B[1;32m   3709\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n",
       "\u001B[1;32m   3710\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n",
       "\u001B[1;32m   3711\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n",
       "\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:1636\u001B[0m, in \u001B[0;36mDistributedDataParallel.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1631\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistributedDataParallel.forward\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m   1632\u001B[0m     inputs, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_forward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1633\u001B[0m     output \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m   1634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1635\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_delay_all_reduce_all_params\n",
       "\u001B[0;32m-> 1636\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_ddp_forward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1637\u001B[0m     )\n",
       "\u001B[1;32m   1638\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_forward(output)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:1454\u001B[0m, in \u001B[0;36mDistributedDataParallel._run_ddp_forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1452\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1453\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inside_ddp_forward():\n",
       "\u001B[0;32m-> 1454\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n",
       "\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/peft/peft_model.py:1719\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1717\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m   1718\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n",
       "\u001B[0;32m-> 1719\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model(\n",
       "\u001B[1;32m   1720\u001B[0m             input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n",
       "\u001B[1;32m   1721\u001B[0m             attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n",
       "\u001B[1;32m   1722\u001B[0m             inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n",
       "\u001B[1;32m   1723\u001B[0m             labels\u001B[38;5;241m=\u001B[39mlabels,\n",
       "\u001B[1;32m   1724\u001B[0m             output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n",
       "\u001B[1;32m   1725\u001B[0m             output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n",
       "\u001B[1;32m   1726\u001B[0m             return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n",
       "\u001B[1;32m   1727\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n",
       "\u001B[1;32m   1728\u001B[0m         )\n",
       "\u001B[1;32m   1730\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n",
       "\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1732\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n",
       "\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001B[0m, in \u001B[0;36mBaseTuner.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n",
       "\u001B[0;32m--> 197\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1163\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1160\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n",
       "\u001B[1;32m   1162\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n",
       "\u001B[0;32m-> 1163\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n",
       "\u001B[1;32m   1164\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n",
       "\u001B[1;32m   1165\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n",
       "\u001B[1;32m   1166\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n",
       "\u001B[1;32m   1167\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n",
       "\u001B[1;32m   1168\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n",
       "\u001B[1;32m   1169\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n",
       "\u001B[1;32m   1170\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n",
       "\u001B[1;32m   1171\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n",
       "\u001B[1;32m   1172\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n",
       "\u001B[1;32m   1173\u001B[0m     cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n",
       "\u001B[1;32m   1174\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n",
       "\u001B[1;32m   1175\u001B[0m )\n",
       "\u001B[1;32m   1177\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[1;32m   1178\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n",
       "\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:913\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n",
       "\u001B[1;32m    901\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n",
       "\u001B[1;32m    902\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n",
       "\u001B[1;32m    903\u001B[0m         hidden_states,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    910\u001B[0m         position_embeddings,\n",
       "\u001B[1;32m    911\u001B[0m     )\n",
       "\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 913\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m decoder_layer(\n",
       "\u001B[1;32m    914\u001B[0m         hidden_states,\n",
       "\u001B[1;32m    915\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mcausal_mask,\n",
       "\u001B[1;32m    916\u001B[0m         position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n",
       "\u001B[1;32m    917\u001B[0m         past_key_value\u001B[38;5;241m=\u001B[39mpast_key_values,\n",
       "\u001B[1;32m    918\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n",
       "\u001B[1;32m    919\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n",
       "\u001B[1;32m    920\u001B[0m         cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n",
       "\u001B[1;32m    921\u001B[0m         position_embeddings\u001B[38;5;241m=\u001B[39mposition_embeddings,\n",
       "\u001B[1;32m    922\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mflash_attn_kwargs,\n",
       "\u001B[1;32m    923\u001B[0m     )\n",
       "\u001B[1;32m    925\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[1;32m    927\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n",
       "\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:656\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    654\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n",
       "\u001B[1;32m    655\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n",
       "\u001B[0;32m--> 656\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(hidden_states)\n",
       "\u001B[1;32m    657\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n",
       "\u001B[1;32m    659\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n",
       "\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:242\u001B[0m, in \u001B[0;36mLlamaMLP.forward\u001B[0;34m(self, x)\u001B[0m\n",
       "\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n",
       "\u001B[0;32m--> 242\u001B[0m     down_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_proj(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgate_proj(x)) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mup_proj(x))\n",
       "\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m down_proj\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n",
       "\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:484\u001B[0m, in \u001B[0;36mLinear4bit.forward\u001B[0;34m(self, x)\u001B[0m\n",
       "\u001B[1;32m    480\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dtype)\n",
       "\u001B[1;32m    482\u001B[0m bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dtype)\n",
       "\u001B[0;32m--> 484\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bnb\u001B[38;5;241m.\u001B[39mmatmul_4bit(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mt(), bias\u001B[38;5;241m=\u001B[39mbias, quant_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mquant_state)\u001B[38;5;241m.\u001B[39mto(inp_dtype)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:533\u001B[0m, in \u001B[0;36mmatmul_4bit\u001B[0;34m(A, B, quant_state, out, bias)\u001B[0m\n",
       "\u001B[1;32m    531\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
       "\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m MatMul4Bit\u001B[38;5;241m.\u001B[39mapply(A, B, out, bias, quant_state)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/autograd/function.py:574\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n",
       "\u001B[1;32m    572\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n",
       "\u001B[1;32m    573\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n",
       "\u001B[0;32m--> 574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n",
       "\u001B[1;32m    577\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n",
       "\u001B[1;32m    578\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    579\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    581\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    582\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:462\u001B[0m, in \u001B[0;36mMatMul4Bit.forward\u001B[0;34m(ctx, A, B, out, bias, quant_state)\u001B[0m\n",
       "\u001B[1;32m    458\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mempty(A\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m B_shape[:\u001B[38;5;241m1\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mdevice)\n",
       "\u001B[1;32m    460\u001B[0m \u001B[38;5;66;03m# 1. Dequantize\u001B[39;00m\n",
       "\u001B[1;32m    461\u001B[0m \u001B[38;5;66;03m# 2. MatmulnN\u001B[39;00m\n",
       "\u001B[0;32m--> 462\u001B[0m output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mlinear(A, F\u001B[38;5;241m.\u001B[39mdequantize_4bit(B, quant_state)\u001B[38;5;241m.\u001B[39mto(A\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39mt(), bias)\n",
       "\u001B[1;32m    464\u001B[0m \u001B[38;5;66;03m# 3. Save state\u001B[39;00m\n",
       "\u001B[1;32m    465\u001B[0m ctx\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m quant_state\n",
       "\n",
       "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 23.73 GiB of which 120.12 MiB is free. Process 104092 has 21.48 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 166.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "OutOfMemoryError",
        "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 23.73 GiB of which 120.12 MiB is free. Process 104092 has 21.48 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 166.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>OutOfMemoryError</span>: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 23.73 GiB of which 120.12 MiB is free. Process 104092 has 21.48 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 166.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-557281251197422>, line 11\u001B[0m\n\u001B[1;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SFTTrainer(\n\u001B[1;32m      3\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m      4\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mdataset,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_arguments,\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:460\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    442\u001B[0m     active_session_failed\n\u001B[1;32m    443\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m autologging_is_disabled(autologging_integration)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;66;03m# warning behavior during original function execution, since autologging is being\u001B[39;00m\n\u001B[1;32m    455\u001B[0m     \u001B[38;5;66;03m# skipped\u001B[39;00m\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001B[1;32m    457\u001B[0m         disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    458\u001B[0m         reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    459\u001B[0m     ):\n\u001B[0;32m--> 460\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    462\u001B[0m \u001B[38;5;66;03m# Whether or not the original / underlying function has been called during the\u001B[39;00m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# execution of patched code\u001B[39;00m\n\u001B[1;32m    464\u001B[0m original_has_been_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/huggingface_patches/transformers.py:54\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_fit_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 54\u001B[0m     model \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     55\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/trainer.py:2164\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2162\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[1;32m   2165\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   2166\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[1;32m   2167\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m   2168\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[1;32m   2169\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/trainer.py:2524\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2517\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2518\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2520\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2521\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2522\u001B[0m )\n\u001B[1;32m   2523\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2524\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n\u001B[1;32m   2526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2527\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2528\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2529\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2530\u001B[0m ):\n\u001B[1;32m   2531\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2532\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/trainer.py:3654\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3654\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs, num_items_in_batch\u001B[38;5;241m=\u001B[39mnum_items_in_batch)\n\u001B[1;32m   3656\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3657\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3658\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3659\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3660\u001B[0m ):\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/trainer.py:3708\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3706\u001B[0m         loss_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n\u001B[1;32m   3707\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs}\n\u001B[0;32m-> 3708\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[1;32m   3709\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   3710\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   3711\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:1636\u001B[0m, in \u001B[0;36mDistributedDataParallel.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1631\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistributedDataParallel.forward\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1632\u001B[0m     inputs, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_forward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1633\u001B[0m     output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1635\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_delay_all_reduce_all_params\n\u001B[0;32m-> 1636\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_ddp_forward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1637\u001B[0m     )\n\u001B[1;32m   1638\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_forward(output)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:1454\u001B[0m, in \u001B[0;36mDistributedDataParallel._run_ddp_forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1452\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1453\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inside_ddp_forward():\n\u001B[0;32m-> 1454\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/peft/peft_model.py:1719\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[1;32m   1717\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1718\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[0;32m-> 1719\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model(\n\u001B[1;32m   1720\u001B[0m             input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1721\u001B[0m             attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m   1722\u001B[0m             inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m   1723\u001B[0m             labels\u001B[38;5;241m=\u001B[39mlabels,\n\u001B[1;32m   1724\u001B[0m             output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m   1725\u001B[0m             output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m   1726\u001B[0m             return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1727\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   1728\u001B[0m         )\n\u001B[1;32m   1730\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1732\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001B[0m, in \u001B[0;36mBaseTuner.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m--> 197\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1163\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m   1160\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m   1162\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m-> 1163\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[1;32m   1164\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1165\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m   1166\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   1167\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m   1168\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m   1169\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m   1170\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m   1171\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m   1172\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1173\u001B[0m     cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n\u001B[1;32m   1174\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   1175\u001B[0m )\n\u001B[1;32m   1177\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1178\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:913\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    901\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    902\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    903\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    910\u001B[0m         position_embeddings,\n\u001B[1;32m    911\u001B[0m     )\n\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 913\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m decoder_layer(\n\u001B[1;32m    914\u001B[0m         hidden_states,\n\u001B[1;32m    915\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mcausal_mask,\n\u001B[1;32m    916\u001B[0m         position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m    917\u001B[0m         past_key_value\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m    918\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    919\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m    920\u001B[0m         cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n\u001B[1;32m    921\u001B[0m         position_embeddings\u001B[38;5;241m=\u001B[39mposition_embeddings,\n\u001B[1;32m    922\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mflash_attn_kwargs,\n\u001B[1;32m    923\u001B[0m     )\n\u001B[1;32m    925\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    927\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:656\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    654\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    655\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n\u001B[0;32m--> 656\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(hidden_states)\n\u001B[1;32m    657\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    659\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:242\u001B[0m, in \u001B[0;36mLlamaMLP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 242\u001B[0m     down_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_proj(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgate_proj(x)) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mup_proj(x))\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m down_proj\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:484\u001B[0m, in \u001B[0;36mLinear4bit.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    480\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dtype)\n\u001B[1;32m    482\u001B[0m bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dtype)\n\u001B[0;32m--> 484\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bnb\u001B[38;5;241m.\u001B[39mmatmul_4bit(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mt(), bias\u001B[38;5;241m=\u001B[39mbias, quant_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mquant_state)\u001B[38;5;241m.\u001B[39mto(inp_dtype)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:533\u001B[0m, in \u001B[0;36mmatmul_4bit\u001B[0;34m(A, B, quant_state, out, bias)\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m MatMul4Bit\u001B[38;5;241m.\u001B[39mapply(A, B, out, bias, quant_state)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/autograd/function.py:574\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    573\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    578\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    579\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    582\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-c3d1da74-cd11-4aaf-a19a-50b7b3d9405a/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:462\u001B[0m, in \u001B[0;36mMatMul4Bit.forward\u001B[0;34m(ctx, A, B, out, bias, quant_state)\u001B[0m\n\u001B[1;32m    458\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mempty(A\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m B_shape[:\u001B[38;5;241m1\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mA\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    460\u001B[0m \u001B[38;5;66;03m# 1. Dequantize\u001B[39;00m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;66;03m# 2. MatmulnN\u001B[39;00m\n\u001B[0;32m--> 462\u001B[0m output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mlinear(A, F\u001B[38;5;241m.\u001B[39mdequantize_4bit(B, quant_state)\u001B[38;5;241m.\u001B[39mto(A\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39mt(), bias)\n\u001B[1;32m    464\u001B[0m \u001B[38;5;66;03m# 3. Save state\u001B[39;00m\n\u001B[1;32m    465\u001B[0m ctx\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m quant_state\n",
        "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 23.73 GiB of which 120.12 MiB is free. Process 104092 has 21.48 GiB memory in use. Of the allocated memory 20.91 GiB is allocated by PyTorch, and 166.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb82f80-7536-48da-97b0-01dc158a8c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your log directory might be ephemeral to the cluster, which will be deleted after cluster termination or restart. You can choose a log directory under /dbfs/ or /Volumes/ to persist your logs in DBFS or UC Volumes.\nTensorboard may not be displayed in the notebook cell output when 'Third-party iFraming prevention' is disabled. You can still use Tensorboard by clicking the link below to open Tensorboard in a new tab. To enable Tensorboard in notebook cell output, please ask your workspace admin to enable 'Third-party iFraming prevention'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin-bottom: 16px\">\n",
       "            <a href=\"/driver-proxy/o/1937444221758252/1127-103433-oashhdnb/6006/\">\n",
       "                Open in a new tab\n",
       "            </a>\n",
       "            <span style=\"margin-left: 1em; color: #a3a3a3\">Note: TensorBoard is only available when this notebook remains attached to the cluster.</span>\n",
       "        </div>\n",
       "        <div style=\"margin-bottom: 16px\">\n",
       "            <span style=\"color: #a3a3a3\">Note: This cell needs to be re-run for TensorBoard to be available if this notebook is imported into a different workspace.</span>\n",
       "        </div>\n",
       "        <iframe id=\"%tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\" src=\"/driver-proxy/o/1937444221758252/1127-103433-oashhdnb/6006/\"></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb5bc22-7155-44d8-9f53-a10a904c1028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b05de8-74ea-4ae2-9166-e0e023e1b9fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What is a large language model? [/INST] A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text, and is designed to generate human-like language. This can include tasks such as text generation, language translation, and chatbots. These models are typically trained on large datasets of text, and are designed to learn patterns and relationships in language.\n\nSome examples of large language models include:\n\n* BERT (Bidirectional Encoder Representations from Transformers): A popular language model that is trained on a large dataset of text, and is designed to generate human-like language.\n* RoBERTa (Robustly Optimized BERT Pretraining Approach): A variant of BERT that is trained on a larger dataset of text, and is designed to generate even more human-like language.\n* Longformer (Long-range dependence transform\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132e0fdb-79d9-414f-8f85-6581a99caf46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fine_tune_Llama7B",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}